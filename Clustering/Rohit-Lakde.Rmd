---
title: "CS 422 Section 01"
author: "Rohit Lakde"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    df_print: paged
    toc: yes
    toc_float: yes
---

## 2.1 Problem 1: K-means clustering
##### Part loading all libraries and setting working directory
```{r}
setwd("/Users/rohit/Documents/")
library(cluster)
library(factoextra)
library(dplyr)
library(ggfortify)
library(ggplot2)
library(NbClust)
```
##### Part Whether transforming the data will help in any way or whether you can use the data in the form
```{r}
UserReview <- read.csv('buddymove_holidayiq.csv',header=T,sep=",")
UserReviewNoid <- UserReview
UserReviewNoid$User.Id <- NULL
UserReviewNoid.s <-scale(UserReviewNoid) 
UserReviewNoid.s
```
> As row User.Id is non numeric nominal attribute so i am removing it, Apart from that range of sports column is 2 to 25 and for religious it is 50 to 203 so it is good to scale the data 

##### Part 2.1 A How many clusters to select?Use fviz_nbclust() to determine the optimum number of clusters
```{r}
k<-list()
betweenSSByTotalSS <- list()
for(i in 1:6){
k[[i]] <- kmeans(UserReviewNoid.s,i)  
}
for(i in 1:6){
betweenSSByTotalSS[[i]]<-k[[i]]$betweenss/k[[i]]$totss 
}
plot(1:6,betweenSSByTotalSS,type="b",ylab="Between SS by Total SS",xlab="Clusters(k)")
fviz_nbclust(UserReviewNoid.s, kmeans, method="wss")
```
>If we have a look at Total WSS vs number of clustes, Between SS vs Total SS then we come to conclusion that after two,three or four clusters there is not reduce significant SS. Even silhouette method given optimal number of clusters as two, SO i am selecting value of k as threebut if we have a look at remaining two graphs then value of k mostly probably is 3

##### Part 2.1 B Run k-means clustering on the dataset to create that many clusters. Plot the clusters using fviz_cluster().

```{r}
Kmeansresult <- kmeans(UserReviewNoid.s,3)
fviz_cluster(Kmeansresult,data=UserReviewNoid.s)
```
> CLusters are created and plotted in above graphs.

##### Part 2.1 C How many observations are in each cluster?
```{r}
print(Kmeansresult['size'])
```
> 42 observations in cluster one, 109 observations in cluster two and 98 observations in cluster three

##### Part 2.1 D What is the total SSE of the clusters?
```{r}
print(Kmeansresult$totss)

```
> Total SSE value is 1488

##### Part 2.1 E What is the SSE of each cluster?
```{r}
print(Kmeansresult$withinss)

```
> SSE of each cluster is 184.7223 213.5241 255.2145

##### Part 2.1 F Perform an analysis of each cluster
```{r}
for (cntr in 1:3){
 print(which(Kmeansresult$cluster == cntr))
 print("\n")
}
```
> If we have a look at above distribution of points then we will find out that clsters formed by method is nature and picnic and it makes sense people who goes to picnic loves nature, Another cluster formed by it is Theatre with shopping again it makes sense, SO i think it is working properly 


## 2.2 Problem : Hierarchical clustering
#####Set seed equals to 1122 take 50 random records and scale data
```{r}
set.seed(1122)
holidayreview <- read.csv("buddymove_holidayiq.csv", header=T, sep=",")
holidayreview <- sample_n(holidayreview,50)
rownames(holidayreview) <- holidayreview[,1]
holidayreview <- holidayreview [ , -1]
holidayreview <- scale(holidayreview)
holidayreview
```

#####2.2 A Run hierarchical clustering on the dataset using factoextra::eclust() method, use k=1 parameter to force a single cluster. Run the clustering algorithm for three linkages: single, complete, and average. Plot the dendogram associated with each linkage using fviz_dend(). Make sure that the labels (User IDs) are visible at the leafs of the dendogram. 
```{r}
selecteddata <- holidayreview

dendo.single <- eclust(selecteddata, "hclust" ,k=1, hc_method="single")
fviz_dend(dendo.single, palette="jco", as.ggplot=T)
dendo.complete <- eclust(selecteddata, "hclust" ,k=1, hc_method="complete")
fviz_dend(dendo.complete,palette="jco", as.ggplot=T)
dendo.average <- eclust(selecteddata, "hclust" ,k=1, hc_method="average")
fviz_dend(dendo.average, palette="jco", as.ggplot=T)
```
#####2.2 B Examine each graph produced in (a) and understand the dendrogram. Notice which users are clustered together as two-singleton clusters (i.e., two users are clustered together because they are very close to each other in the attributes they share). For each linkage method, list all the two-singleton clusters. For instance, {User 43,User 35} form a two-singleton cluster in the average linkage method since they share a lot of the same characteristics

>Answers

For single linkage: {200,195},{224,221},{167,240},{71,98},{18,41},{12,11},{36,60},{23,38},{43,35},{157,131},{116,139},{140,145},{155,136},{197,217},{199,168}

For complete linkage: {71,98},{12,11},{72,73},{115,56},{18,41},{23,38},{43,35},{36,60},{4,37},{200,195},{199,168},{197,217},{224,221},{167,240},{140,145},{155,136},{170,225},{157,131},{116,139}

For average linkage: {12,11},{43,35},{36,60},{4,37},{72,73},{18,41},{23,38},{71,98},{140,145},{155,136},{157,131},{116,139},{200,195},{224,221},{167,240},{197,217},{170,225},{199,168}

#####2.2 C We will now determine how many clusters to form. Let's pick a hierarchical cluster that we will call pure,and let's define purity as the linkage strategy that produces the least two-singleton clusters. Of the linkage methods you examined in (b), which linkage method would be considered pure by our definition?
> As number of two singleton clusters in single linkage are minimal that are 15,So single linkage method would be considered pure by given strategy.  

#####2.2 D Using the graph corresponding to the linkage method you chose in (c), draw a horizontal line at a height of 1.7. How many clusters would you have?
```{r}
clust17 <- cutree(dendo.single, h=1.7)
sprintf("We have %d Clusters", max(clust17))
```
#####2.2 E  For the number of clusters you determined in (d), re-run hierarchical clustering across the three linkage strategies (single, average, and complete) with the value of k being the number of clusters you determined in (d). For each linkage strategy, find out its Silhouette index.
```{r}
newdendosingle <- eclust(holidayreview, "hclust" , k = max(clust17),  hc_method="single")
fviz_dend(newdendosingle, palette="jco", as.ggplot=T)

newdendocomplete <- eclust(holidayreview, "hclust", k = max(clust17), hc_method="complete")
fviz_dend(newdendosingle,palette="jco", as.ggplot=T)

newdendoaverage <- eclust(holidayreview, "hclust", k= max(clust17), hc_method="average")
fviz_dend(newdendosingle, palette="jco", as.ggplot=T)

stats <- fpc::cluster.stats(dist(holidayreview), cutree(newdendosingle, k=max(clust17)))
sprintf("For Single Link Silwidth %f", stats['avg.silwidth'])

stats <- fpc::cluster.stats(dist(holidayreview), cutree(newdendocomplete, k=max(clust17)))
sprintf("For complete Link Silwidth %f", stats['avg.silwidth'])

stats <- fpc::cluster.stats(dist(holidayreview), cutree(newdendoaverage, k=max(clust17)))
sprintf("For Average Link Silwidth %f", stats['avg.silwidth'])
```
#####2.2 F  For each linkage strategy, determine the number of clusters that NbClust() suggests. Take a look at the method= parameter in NbClust() and pass in the linkage method, e.g.: NbClust(data, method="single").
```{r}
NbClust(holidayreview,method="single")
sprintf("For Single number of clusters are 9")
NbClust(holidayreview,method="complete")
sprintf("For Complete number of clusters are 2")
NbClust(holidayreview,method="average")
sprintf("For Average number of clusters are 2")
```
#####2.2 g For the number of clusters you determined for each linkage in (f), find out its Silhouette index
```{r}
NbClust(holidayreview, method ="single",index="silhouette")
sprintf("Silhouette index for single link is 0.3382 ")
NbClust(holidayreview, method ="complete",index="silhouette")
sprintf("Silhouette index for complete link is 0.3992 ")
NbClust(holidayreview, method ="average",index="silhouette")
sprintf("Silhouette index for average link is 0.4173")

```
#####2.2 h   You used two strategies to cluster: one was in part (c) where you used purity to define clusters, and the other strategy was using NbClust(). Between these two strategies, pick the linkage that is best, as defined by the Silhouette index. Comment on which of the two strategies comes closest to your expectations.
> If we compare average purity by silhouette is 0.3992433 and For NbClust average index mentioned is 0.3849, So according to me purity based method is good 

## 2.3 Problem : K-Means & PCA 
#####Read file
```{r}
HtruData <- read.csv('HTRU_2-small.csv',header=T,sep=",")
HtruDataScaled <- scale(HtruData)
```
#####2.3 A Perform PCA on the dataset
```{r}
HtruPCA<-prcomp(HtruDataScaled)
```
#####2.3 A(i) How much cumulative variance is explained by the first two components?
```{r}
summary(HtruPCA)
```
> Variance explained by first two components is 0.7696

#####2.3 A(ii)  Plot the first two principal components. Use a different color to represent the observations in the two classes
```{r}
autoplot(HtruPCA, data = HtruData, colour = "class", loadings = TRUE,loadings.colour = 'blue',loadings.label = TRUE)
```
#####2.3 A(iii) Describe what you see with respect to the actual label of the HTRU2 dataset.
> By observing above graph we can say that eighen vectors for skewness, kurtosis and skewness.dm.snr, kurtosis.dm.snr are in same direction from that we can say that skewness, kurtosis and skewness.dm.snr, kurtosis.dm.snr are highly correlated.

#####2.3 B Perform K-Means on above dataset
#####2.3 B(i) Perform K-means clustering on the dataset with centers = 2, and nstart = 25 
```{r}
HtrKMeans <- kmeans(HtruDataScaled, centers=2, nstart=25)
fviz_cluster(HtrKMeans, data=HtruData)
```

#####2.3 B(ii)Provide observations on the shape of the clusters you got in (b)(i) to the plot of the first two principal components in (a)(ii). If the clusters are are similar, why? If they are not, why?
> Clusters are similar but not same as PCA considers only few attributes in above case 2 which explains highest standard deviation whereas k-means considers all the available attributes

#####2.3 B(iii) What is the distribution of the observations in each cluster?
```{r}
sprintf("Distribution of cluster is ")
HtrKMeans$size
```

#####2.3 B(iV) What is the distribution of the classes in the HTRU2 dataset? 
```{r}
class1 <- sum(HtruData$class)
sprintf(" Class 1 have %d Obs" , class1)
sprintf(" Class 0 have %d Obs" , dim(HtruData)[1] - class1)

```
#####2.3 B(V)  Based on the distribution of the classes in (b)(iii) and (b)(iv), which cluster do you think corresponds to the majority class and which cluster corresponds to the minority class?
> Based on distribution of classes in b(iii) and b(iV), Cluster 1 corresponds to majority class that is class 0.

#####2.3 B(Vi)   Let's focus on the larger cluster. Get all of the observations that belong to this cluster. Then, state what is the distribution of the classes within this large cluster; i.e., how many observations in this large cluster belong to class 1 and how many belong to class 0?
```{r}
HtruDataC1 <- HtruData[HtrKMeans$cluster==1,]
num <- sum(HtruDataC1$class)
sprintf(" Number of obs belonging to class 1 in large cluster %d" , num)
sprintf(" Number of obs belonging to class 0 in large cluster %d" , dim(HtruDataC1)[1] - num)
```
#####2.3 B(Vii)Based on the analysis above, which class (1 or 0) do you think the larger cluster represents?
> Based on above analysis larger cluster represents class 0 

#####2.3 B(Viii)How much variance is explained by the clustering?
```{r}
clusplot(HtruDataScaled, HtrKMeans$cluster)
```
> Variance explained by clustering is 76.97%

#####2.3 B(iX)What is the average Silhouette width of both the clusters?
```{r}
SilOfHtr<- silhouette(HtrKMeans$cluster, dist(HtruData))
summary(SilOfHtr)
```
> Average silhouette widths of both clusters is 0.2814450,0.5442131

#####2.3 B(X)What is the per cluster Silhouette width? Based on this, which cluster is good?
```{r}
silhswdth <- silhouette(HtrKMeans$cluster,dist(HtruDataScaled))
a<-summary(silhswdth)
a$clus.avg.widths
```
> Silhouette width tells us about how given object is related to its own cluster,For 1st cluster value is 0.6599447 and for second it is 0.3714456 As per given information cluster one has greater average silhouette width so it's a better cluster

#####2.3 C Perform K-means on the result of the PCA you ran in (a). More specifically, perform K-means on the first two principal component score vectors (i.e., pca$x[, 1:2]). Use k = 2.
```{r}
kout <- kmeans(HtruPCA$x[, 1:2], centers=2, nstart=25)
```

#####2.3 C(i) Plot the clusters and comment on their shape with respect to the plots of a(ii) and b(i).
```{r}
fviz_cluster(kout, data=HtruData)
```
> Shape of plots in a(ii), b(i) and c(i) is similar and almost same

#####2.3 C(ii) What is the average Silhouette width of both the clusters?
```{r}
NewSil<- silhouette(kout$cluster, dist(HtruData))
summary(NewSil)
```
> Average silhouette widths for both the clusters is 0.2857006,0.5502426

#####2.3 C(iii)  What is the per cluster Silhouette width? Based on this, which cluster is good?
```{r}
silhswdth1 <- silhouette(kout$cluster,dist(HtruDataScaled))
a1<-summary(silhswdth1)
a1$clus.avg.widths
```
>Silhouette width tells us about how given object is related to its own cluster,For first cluster value is 0.6611024 and for second it is 0.3817566 As per given information cluster one has greater average silhouette width so it's a better cluster

#####2.3 C(iv)   How do the values of c(ii) and c(iii) compare with those of b(ix) and b(x), respectively?
> Values in c(ii),c(iii) and b(ix),b(x) are 0.2857006,0.5502426 and 0.2814450,0.5442131, As average silhouette widths for C are more so average silhouette width for C is better.  






